# ML-Concepts

## Index

1. [Distributions][1].
2. [EDA][2].
3. [KNN Algorithm][3].
4. [Naive Bayes classifier][4].
5. [Logistic Regression][5].
6. [Linear Regression][6].
7. ~~[Support Vector Machine][7].~~
8. [Decision Tree][8]
9. [Random Forest][9]
10. [Boosting][10]

## [1] Distributions

1. Normal (Gaussian) Distribution
2. Central Limit Theorem
3. Q-Q PLot


## [2] EDA: Exploratory Data Analysis

1. EDA of Haberman's Survival dataset
2. EDA of Titanic Mortality dataset.


## [3] K-Nearest Neighbor (KNN)

> Note: Create folder by name `Output` inside `3. K-Nearest Neighbor (KNN)`
folder before funning the notebook.

#### Objective of `K-Nearest Neighbor (KNN).ipynb` notebook:

1. Plot cluster of data-point.
2. Implementing KNN algorithm from scratch.
3. Test the accuracy of the model trained on KNN.


## [4] Naive Bayes classifier

#### Objective of `Naive Bayes Classifier.ipynb` notebook:

1. Implement simple Naive Bayes classifier for categorical features.
2. Train and test model on sample weather forecast dataset using Naive Bayes.

#### Objective of `Naive Bayes Classifier - CategoricalNB.ipynb` notebook:

1. Train Naive Bayes classifier using `CategoricalNB` from `sklearn`.
2. Train and test model on sample weather forecast dataset.

## [5] Logistic Regression

#### Objective of `3. Logistic Regression.ipynb` notebook:

Implement multinomial **Logistic Regression** using _Gradient Descent_ minimization technique.

## [6] Linear Regression

#### Objective of `1. Squared Error Cost Function.ipynb` notebook:

Implement cost function for multivariate _Linear Regression_ based on **Squared Error** technique.

#### Objective of `2. Batch Gradient Descent.ipynb` notebook:

Implement **Batch Gradient Descent** to minimize _Squared Error Cost function_.

#### Objective of `3. Linear Regression - Gradient Descent.ipynb` notebook:

Implement multivariate **Linear Regression** using _Gradient Descent_ minimization technique.

#### Objective of `4. Linear Regression - Normal Equation.ipynb` notebook:

Implement multivariate **Linear Regression** using _Normal Equation_ technique.

## ~~[7] Support Vector Machine (SVM)~~

#### ~~Objective of `1. Linear SVR - Gradient Descent.ipynb` notebook:~~

~~Implement **Support Vector Regression** without any Kernel i.e., linear-SVR.~~

## [8] Decision Tree

#### Objective of `2. Decision Tree - DecisionTreeClassifier.ipynb` notebook

Visualize decision tree


<!-- Permalinks for Index -->
[1]: https://github.com/DheemanthBhat/ML-Concepts/blob/d4fcca7cc6b066e940597daa7f517f205041e1d0/1.%20Distributions/
[2]: https://github.com/DheemanthBhat/ML-Concepts/blob/2493dcdd9445623b135b297781c07543bd691334/2.%20EDA/
[3]: https://github.com/DheemanthBhat/ML-Concepts/blob/2493dcdd9445623b135b297781c07543bd691334/3.%20K-Nearest%20Neighbor%20(KNN)/
[4]: https://github.com/DheemanthBhat/ML-Concepts/blob/2493dcdd9445623b135b297781c07543bd691334/4.%20Naive%20Bayes/
[5]: https://github.com/DheemanthBhat/ML-Concepts/blob/2493dcdd9445623b135b297781c07543bd691334/5.%20Logistic%20Regression/
[6]: https://github.com/DheemanthBhat/ML-Concepts/blob/2493dcdd9445623b135b297781c07543bd691334/6.%20Linear%20Regression/
[7]: #
[8]: https://github.com/DheemanthBhat/ML-Concepts/blob/a2edea0bb6656ecfa068dc80dec065ca5d74da41/08.%20Decision%20Tree/
[9]: #
[10]: https://github.com/DheemanthBhat/ML-Concepts/blob/a2edea0bb6656ecfa068dc80dec065ca5d74da41/10.%20Boosting/